---
layout: post
title: "[텐서플로우] DNN 구현"
categories: ml
---

텐서플로우 로 DNN 을 구현합니다.

* this unordered seed list will be replaced by toc as unordered list
{:toc}

## 데이터 가져오기

```python
import tensorflow as tf
import numpy as np

mnist = tf.keras.datasets.fashion_mnist
(training_images, training_labels), (test_images, test_labels) = mnist.load_data()

training_images = training_images / 255.0
test_images = test_images / 255.0
```

여기서 `255.0`로 나누어 주는 것을 정규화(Normalization)라고 하는데 아래와 같은 효과가 있다.

- 빠른 학습
- local minimum 에 빠질 가능성을 줄여줌

## 모델 만들기

```python
model = tf.keras.models.Sequential([
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(10, activation='softmax')
])
```

- **Sequential**: 신경망에 레이어들을 선형으로 연결
- **Flatten**: 입력값을 1차원 배열로 펼침
- **Dense**: 보통 히든 레이어에 사용. 가중치를 포함. activation function 을 필요로 함
- **Relu**: activation function. X가 0보다 크면 X를 출력 그외는 0 출력
- **Softmax**: 출력값에 대해서 정규화를 해 줌. classification 에서 사용. 결과값의 합은 언제나 1.0

## optimizer와 loss function 선택

```python
model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)
```

- **loss**: 실제값과 예측값의 차이를 계산
- **optimizer**: loss를 낮추도록 파라미터의 값을 변화시키는 방법

## 학습

```python
model.fit(training_images, training_labels, epochs=30)
```

- **epoch**: 학습을 반복할 횟수

## 테스트 데이터로 평가

```python
model.evaluate(test_images, test_labels)
```
