---
layout: post
title: "[수알못을 위한 딥러닝 with Tensorflow 01] 머신러닝"
categories: ml
---

이 포스팅은 `Coursera`의 `Introduction to TensorFlow for Artificial Intelligence, Machine Learning, and Deep Learning` 강의를 정리한 내용입니다.
{:.note}

머신러닝에 대해 알아봅시다.

0. this unordered seed list will be replaced by toc as unordered list
{:toc}

## 프로그래밍 패러다임의 변화

전통적인 프로그래밍 방법은 아래 그림처럼 `입력`과 `로직`을 컴퓨터에게 주었을 때 원하는 `출력`이 나오는 형태입니다. 오늘 날 대부분의 소프트웨어는 이러한 방식으로 동작합니다.

![ml-01](/assets/img/ml-01.jpg)

전통적인 프로그래밍 패러다임
{:.figcaption}

실제 코드를 통해 좀 더 알아볼까요? 아래 코드는 사용자로부터 입력을 받아서 2를 곱하고 1을 뺀 후 그 값을 돌려주는 함수입니다.

```python
def my_func(x: int):
    return x*2-1
```

이러한 프로그래밍 방식은 놀라울만큼 우리 삶에 큰 도움을 주고 있습니다.


그러나, 이러한 방식으로 풀기 힘든 문제들이 존재합니다. 예를 들어, 개와 고양이를 구분하려면 어떻게 로직을 구성해야 할까요? 이를 논리적인 코드로 작성하기엔 이 세상에 너무 많은 고양이와 개들이 존재합니다.

```python
def is_dog(image):
    # ???
```

컴퓨터 과학자들과 수학자들은 이러한 문제를 해결하기 위해 기존의 전통적인 프로그래밍 패러다임을 바꿉니다. 우리가 모르는 `로직`을 컴퓨터의 출력 값으로 둔 것이죠.

![ml-02](/assets/img/ml-02.jpg)

새로운 프로그래밍 패러다임
{:.figcaption}

컴퓨터에게 고양이 사진과 이것은 고양이야 라고 말해주면 (마치 아기를 가르치듯이) 컴퓨터는 고양이 사진을 구분하는 `로직`을 내뱉어 주는거죠. 놀랍지 않나요?


물론 아기한테 하는 것처럼 컴퓨터에게 웹캠을 통해 개와 고양이 사진을 보여줄 수는 없겠죠? 컴퓨터에게 `학습`을 시키기 위한 다양한 수학적 통계학적 방법이 있습니다. 수학적 배경이 탄탄하다면 머신러닝을 이해하고 응용하는데 큰 도움이 되겠지만, 사실 몰라도 이를 구현하고 적용하는 데 전혀 문제가 없습니다.


수학적 배경 없이도 프로그래밍만 할 줄 안다면 딥러닝 알고리즘을 구현할 수 있도록 하나씩 같이 배워봅시다.

## 신경망

![ml-03](/assets/img/ml-03.jpeg)

뉴런
{:.figcaption}

neural network라고 불리는 신경망은 사람의 뇌에서 일어나는 정보 전달 방식을 모방한 것입니다. 하나의 뉴런은 들어오는 입력에 대해서 w라는 `가중치` 값을 곱하고 출력으로 내보내는데, 이러한 뉴런이 여러 개, 여러 층으로 구성된 것을 `인공 신경망` 이라고 합니다.


## 머신러닝을 통해 `2x-1` 학습하기

우리는 `파이썬`이라는 프로그래밍 언어를 통해 딥러닝 알고리즘들을 구현합니다. 또 다양한 파이썬 라이브러리를이 우리릍 도와줄 예정인데요.


먼저 `tensorflow`라는 파이썬 라이브러리는 구글에서 개발한 머신러닝 프레임워크로써, 수많은 머신 러닝 알고리즘들이 내장되어 있고 이를 쉽게 이용할 수 있도록 되어 있습니다. `numpy` 라이브러리는 배열, 행렬 계산을 위해 개발된 수치 연산 라이브러리 입니다.


`pip`를 통해 위 라이브러리들을 설치합시다.

```
pip install tensorflow numpy
```

이제 위에서 살펴본 `2x-1` 방정식을 머신러닝 프로그래밍 방식으로 구현해봅시다.


먼저 모델을 만들어야겠죠? `tensorflow`의 `keras` API를 통해 뉴런이 1개인 매우 간단한 모델을 만들어봅시다.

```python
model = tf.keras.Sequential([tf.keras.layers.Dense(units=1, input_shape=[1])])
```

`tf.keras.Sequential` 은 신경망 층을 연속적으로 구성할 때 사용합니다. 우리는 이번에 한 개층만 만들겁니다. `tf.keras.layers.Dense`는 신경망 층의 한 종류라고 보면 되는데, 모든 뉴런들이 연결되어 있는 형태입니다. 뉴런이 한 개이기 떄문에 `units` 값을 1로 두었고, 입력도 1개 이기 떄문에 `input_shape`도 크기 1의 1차원 형태입니다.


모델은 다 구성하였고, 어떻게 컴퓨터가 학습을 할지 도와줘야 하겠죠?

```python
model.compile(optimizer='sgd', loss='mean_squared_error')
```

`model.compile` 함수를 통해 구성된 모델을 어떻게 학습할지 정할 수 있습니다. 여기서 `loss`는 컴퓨터가 예측한 값과 실제 값이 얼마나 차이나는 지 계산하는 함수입니다. 이 값이 작을수록 컴퓨터가 제대로 학습한 것이겠죠? `optimizer`는 loss를 통해 컴퓨터가 만든 예측 로직을 어떻게 개선할지 정하는 함수입니다.


학습을 위한 준비는 끝났습니다. 이제 컴퓨터에게 줄 `입력`과 `출력` 데이터셋을 만듭시다.

```python
xs = np.array([-1.0,  0.0, 1.0, 2.0, 3.0, 4.0], dtype=float)
ys = np.array([-3.0, -1.0, 1.0, 3.0, 5.0, 7.0], dtype=float)
```

`xs`는 입력 값으로 6개의 값으로 구성되어 있습니다. `ys`는 이에 대한 정답 데이터셋입니다. 우리는 `2x-1` 방정식을 학습시키기로 하였기 때문에 -1에 대해서는 -3이, 2에 대해서는 3이 나와야 됩니다.


이제 컴퓨터에게 데이터셋을 주고 학습을 시킵시다.

```python
model.fit(xs, ys, epochs=500)
```

`fit` 함수는 우리가 만든 모델과 데이터셋으로 학습을 시킬 때 사용합니다. `epoch`는 학습 횟수 입니다. 사람에게 치면 수학 문제집을 반복해서 푸는 횟수가 되겠네요.


![ml-04](/assets/img/ml-04.png)

학습 과정
{:.figcaption}

학습 과정을 보면 `epoch`가 증가하면서, 즉 학습을 거듭하면서 `loss`가 줄어드는 것을 볼 수 있죠. 이는 컴퓨터가 예측한 값과 우리가 `ys`로 전달한 값의 오차가 점점 줄어드는 것을 의미합니다. `loss`가 감소하는 것을 보니 컴퓨터가 제대로 학습을 하는 것 같군요.


이제 `predict` 함수를 통해 컴퓨터에게 새로운 값을 전달해봅시다. 10이라는 값을 전달하면 `2x-1` 가 제대로 학습되었다면 19라는 값이 나와야하겠죠?


![ml-05](/assets/img/ml-05.png)

예측 결과
{:.figcaption}

19에 근사한 값이 나왔습니다. 컴퓨터 제대로 학습한 모양이네요.

## 정리

오늘은 머신러닝에 대한 기초와 새롭게 바뀐 프로그래밍 패러다임에 알아봤습니다. 다음에는 좀 더 어려운 문제를 머신 러닝으로 풀어보도록 하겠습니다.